<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9NWBV84HB2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9NWBV84HB2');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Kyle Vedder's Homepage">
<meta name="author" content="Kyle Vedder">
<link rel="shortcut icon" href="./favicon.ico">
<title>
Kyle Vedder’s Homepage
</title>
<!-- css -->
<link href="./css/style.css" rel="stylesheet"> <!-- JavaScript -->
<script type="text/javascript" src="./js/utils.js"></script>
</head>
<h1 class="centered">
I’m Kyle Vedder
</h1>
<p><img class="centered" src="img/me_outside.jpg" height="400" /></p>
<p>I believe the shortest path to getting robust, generally capable robots in the real world is through the construction of <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">systems whose performance scales with compute and data, <em>without</em> requiring human annotations</a>.</p>
<p>In service of this, I am interested in designing and scaling fundamentally 3D vision systems that learn just from raw, multi-modal data. My contrarian bet is on the multi-modal and 3D aspects; a high quality, 3D aware representation with diverse data sources should enable more sample efficient and robust downstream policies. Most representations today are 2D for historical reasons (e.g. lots of RGB data, 2D convolutions won the hardware lottery), but I believe this ends up pushing a lot of 3D spacial understand out of the visual representation and into the downstream policy, making them more expensive to learn and less robust.</p>
<p>For data availability reasons, <a href="http://vedder.io/zeroflow.html">my current work</a> is in the Autonomous Driving domain, but I believe the same principles apply to other domains such as indoor service robots.</p>
<h2 id="background">Background</h2>
<p>I am a CS PhD <a href="img/static/candidate.png">candidate</a> at Penn under <a href="https://www.seas.upenn.edu/~eeaton/">Eric Eaton</a> and <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a> in the <a href="https://www.grasp.upenn.edu/">GRASP Lab</a>. My current work lies in:</p>
<ul>
<li><a href="./zeroflow.html">Finding label-free, scalable 3D problem formulations</a></li>
<li><a href="./sparse_point_pillars.html">Designing efficient 3D perception systems</a></li>
</ul>
<p>During my undergrad in CS at UMass Amherst I did research under <a href="https://www.joydeepb.com/">Joydeep Biswas</a> in the <a href="https://amrl.cs.umass.edu/">Autonomous Mobile Robotics Lab</a>. My research was in:</p>
<ul>
<li><a href="http://vedder.io/publications/ScaffoldsLaneVedderBiswasPlanRob2017.pdf">Single-Agent Path Finding (SAPF)</a> for sampling based planners</li>
<li><a href="./xstar.html">Anytime Multi-Agent Path Finding (MAPF)</a> for efficient first solution generation</li>
<li><a href="http://vedder.io/publications/MinutebotsRoboCupTDP2017.pdf">Core infrastructure</a> and <a href="http://vedder.io/publications/MinutebotsRoboCupTDP2018.pdf">low level safety system</a> of our <a href="https://amrl.cs.umass.edu/minutebots.html">RoboCup Small Size League team</a></li>
</ul>
<p>I have also done a number of industry internships; I have interned twice at Unidesk (a startup since aquired by Citrix), twice at Google, once at Amazon’s R&amp;D lab, Lab126 (where I worked on their home robot <a href="https://www.aboutamazon.com/news/devices/meet-astro-a-home-robot-unlike-any-other">Astro</a>), and at <a href="https://www.argo.ai/">Argo AI</a> as a Research Intern under <a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a>.</p>
<h2 id="more-information">More Information</h2>
<ul>
<li>Email: kvedder (at) seas.upenn.edu</li>
<li>Resume: <a href="KyleVedderResume.pdf">/resume</a></li>
<li>Publications: <a href="publications.html">/publications</a></li>
<li>GitHub: <a href="https://github.com/kylevedder">kylevedder</a></li>
<li>Twitter: <a href="https://twitter.com/KyleVedder">KyleVedder</a></li>
</ul>
<h2 id="updates">Updates</h2>
<div class="updates">
<ul>
<li>Dec 4th, 2023: <a href="./misc/review_ai_is_good_for_you.html">Book Review: Eric Jang’s book “<em>AI is Good for You</em>”</a></li>
<li>Nov 21st, 2023: I will be joining Nvidia’s AV team as a Research Intern starting in January! I will be extending <a href="http://vedder.io/zeroflow">ZeroFlow</a> in an exciting new way (more to come!)</li>
<li>Aug 3rd, 2023: <a href="https://vedder.io/misc/applying_to_ml_phd.html">Blog post: Applying to CS PhD programs for Machine Learning: what I wish I knew</a></li>
<li>Jul 28th, 2023: <a href="./zeroflow.html">ZeroFlow XL</a> is now <strong>state-of-the-art</strong> on the <a href="https://eval.ai/web/challenges/challenge-page/2010/leaderboard/4759">Argoverse 2 Self-Supervised Scene Flow Leaderboard</a>! And we’ve only begun to scale our method — there is plenty of performance left on the table!</li>
<li>Jul 3rd, 2023: <a href="misc/research_dev_env.html">Blog post: My ML research development environment workflow</a></li>
<li>Jun 18th, 2023: <a href="./zeroflow.html">ZeroFlow</a> was selected as a <strong>highlighted method</strong> in the CVPR 2023 <em>Workshop on Autonomous Driving</em> Scene Flow Challenge!</li>
<li>May 18th, 2023: <a href="./zeroflow.html"><em>ZeroFlow: Fast Zero Label Scene Flow via Distillation</em></a> was submitted.</li>
<li>Jan 12th, 2023: <a href="./publications/L2M_eval_preprint.pdf"><em>A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems</em></a> was accepted to Neural Networks.</li>
<li>Jun 30th, 2022: <a href="./sparse_point_pillars.html"><em>Sparse PointPillars</em></a> was accepted to IROS 2022. <a href="./misc/SparsePointPillars_IROS_2022_reviews.txt">(Reviews)</a></li>
<li>Jun 7th, 2022: <a href="https://www.youtube.com/watch?v=JgcR6cFXR5w">Invited talk for <em>Sparse PointPillars</em> at 3D-DLAD</a></li>
<li>May 15th, 2022: Joined Argo as a Research Intern!</li>
<li>Mar 1st, 2022: <a href="./sparse_point_pillars.html">Submitted <em>Sparse PointPillars</em> to IROS 2022</a></li>
<li>Jul 20th, 2021: <a href="./xstar.html">Added project webpage for X*</a></li>
<li>Jul 8th, 2021: <a href="misc/SparsePointPillarsSNNPoster.pdf">Poster presented at Sparse Neural Networks on <em>Sparse PointPillars</em></a></li>
<li>Jun 14th, 2021: <a href="publications/sparse_point_pillars_snn_workshop.pdf">Workshop paper accepted as poster to Sparse Neural Networks: <em>Sparse PointPillars: Exploiting Sparsity on Birds-Eye-View Object Detection</em></a></li>
<li>Apr 27th, 2021: <a href="https://www.youtube.com/watch?v=xFFCQVwYeec">My WPEII Presentation: <em>Current Approaches and Future Directions for Point Cloud Object Detection in Intelligent Agents</em></a></li>
<li>Apr 14th, 2021: <a href="misc/KyleVedderWPEII2021.pdf">My WPEII Document: <em>Current Approaches and Future Directions for Point Cloud Object Detection in Intelligent Agents</em></a></li>
<li>Feb 11th, 2021: <a href="misc/mujoco_py.html">Blog post: Setting up <code>mujoco-py</code> for use with on-screen and off-screen rendering</a></li>
<li>Nov 4th, 2020: <a href="http://vedder.io/publications/expanding_astar_aij.pdf">Journal paper accepted to Artificial Intelligence: <em>X*: Anytime Multi-Agent Path Finding for Sparse Domains using Window-Based Iterative Repairs</em></a></li>
<li>Jul 23rd, 2020: <a href="https://www.youtube.com/watch?v=4RkhsIz14Yc">Presentation: <em>From Shapley Values to Explainable AI</em></a></li>
<li>Jun 29rd, 2020: <a href="https://www.youtube.com/watch?v=o7WW2cu1h7c">Demo: <em>Penn Service Robots navigating around Levine</em></a></li>
<li>May 8th, 2020: <a href="misc/shap_for_classification.pdf">Term paper: <em>An Overview of SHAP-based Feature Importance Measures and Their Applications To Classification</em></a>
</div></li>
</ul>
